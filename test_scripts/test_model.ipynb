{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer  \u001b[38;5;66;03m# ç”¨äºè‡ªåŠ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†è¯å™¨\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\u001b[34;43m__file__\u001b[39;49m), \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MiniMindLM  \u001b[38;5;66;03m# å¯¼å…¥è‡ªå®šä¹‰çš„æ¨¡å‹ç±» MiniMindLM\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥æ ‡å‡†åº“\n",
    "import os  # ç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’ï¼Œä¾‹å¦‚è¯»å–ç¯å¢ƒå˜é‡ã€åˆ›å»ºç›®å½•\n",
    "import platform  # ç”¨äºè·å–æ“ä½œç³»ç»Ÿå¹³å°ä¿¡æ¯ (è™½ç„¶åœ¨æ­¤è„šæœ¬ä¸­æœªç›´æ¥ä½¿ç”¨å…¶è¿”å›å€¼)\n",
    "import argparse  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°\n",
    "import time  # ç”¨äºè®¡æ—¶ï¼Œä¾‹å¦‚è®¡ç®—è®­ç»ƒæ—¶é•¿\n",
    "import math  # ç”¨äºæ•°å­¦è¿ç®—ï¼Œä¾‹å¦‚è®¡ç®—ä½™å¼¦å­¦ä¹ ç‡\n",
    "import warnings  # ç”¨äºæ§åˆ¶è­¦å‘Šä¿¡æ¯çš„æ˜¾ç¤º\n",
    "\n",
    "# å¯¼å…¥ç¬¬ä¸‰æ–¹åº“\n",
    "import pandas as pd  # æ•°æ®å¤„ç†åº“ (è™½ç„¶åœ¨æ­¤è„šæœ¬ä¸­æœªç›´æ¥ä½¿ç”¨ï¼Œå¯èƒ½åœ¨å¯¼å…¥çš„æ¨¡å—ä¸­ä½¿ç”¨æˆ–ä¸ºæœªæ¥æ‰©å±•ä¿ç•™)\n",
    "import torch  # PyTorchæ ¸å¿ƒåº“ï¼Œç”¨äºå¼ é‡è®¡ç®—å’Œç¥ç»ç½‘ç»œ\n",
    "import torch.distributed as dist  # PyTorchåˆ†å¸ƒå¼è®­ç»ƒåº“ (åœ¨æ­¤æµ‹è¯•è„šæœ¬ä¸­æœªä½¿ç”¨)\n",
    "from torch import optim, nn  # PyTorchä¼˜åŒ–å™¨ (optim) å’Œç¥ç»ç½‘ç»œæ¨¡å— (nn)\n",
    "\n",
    "# from torch.nn.parallel import DistributedDataParallel # ä¸éœ€è¦ DDP è¿›è¡Œå• GPU æ¨ç†\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    ")  # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ (è™½ç„¶æœªä½¿ç”¨ï¼Œä½†å¯¼å…¥äº†)\n",
    "\n",
    "# from torch.utils.data import DataLoader, DistributedSampler # ä¸éœ€è¦æ•°æ®åŠ è½½å™¨è¿›è¡Œç®€å•ç”Ÿæˆ\n",
    "from contextlib import nullcontext  # ç”¨äºåˆ›å»ºç©ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ–¹ä¾¿åœ¨ä¸åŒè®¾å¤‡ç±»å‹é—´åˆ‡æ¢\n",
    "\n",
    "# å¯¼å…¥Hugging Face Transformersåº“\n",
    "from transformers import AutoTokenizer  # ç”¨äºè‡ªåŠ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†è¯å™¨\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
    "# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—\n",
    "from model.model import MiniMindLM  # å¯¼å…¥è‡ªå®šä¹‰çš„æ¨¡å‹ç±» MiniMindLM\n",
    "from model.LMConfig import LMConfig  # å¯¼å…¥è‡ªå®šä¹‰çš„æ¨¡å‹é…ç½®ç±» LMConfig\n",
    "\n",
    "# from model.dataset import PretrainDataset # ä¸éœ€è¦æ•°æ®é›†ç±»è¿›è¡Œç®€å•ç”Ÿæˆ\n",
    "\n",
    "\n",
    "# --- ä½ æä¾›çš„åˆå§‹åŒ–ä»£ç  ---\n",
    "def init_model(lm_config):\n",
    "    # å‡è®¾ä½ çš„tokenizeræ–‡ä»¶ç¡®å®åœ¨ ./model/minimind_tokenizer ç›®å½•ä¸‹\n",
    "    tokenizer_path = \"./model/minimind_tokenizer\"\n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ä½œä¸ºå¤‡é€‰\n",
    "        # æˆ–è€…ç›´æ¥æŠ¥é”™æç¤ºç”¨æˆ·æ£€æŸ¥è·¯å¾„\n",
    "        warnings.warn(\n",
    "            f\"Tokenizer path {tokenizer_path} not found. Trying a placeholder 'bert-base-uncased'. Adjust if needed.\"\n",
    "        )\n",
    "        tokenizer_path = \"bert-base-uncased\"  # æˆ–è€…å…¶ä»–ä½ çŸ¥é“å­˜åœ¨çš„è·¯å¾„, æˆ–è€…æŠ›å‡ºé”™è¯¯\n",
    "        # raise FileNotFoundError(f\"Tokenizer path not found: {tokenizer_path}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    model = MiniMindLM(lm_config)\n",
    "    moe_path = \"_moe\" if lm_config.use_moe else \"\"\n",
    "    ckp = f\"./out/pretrain_{lm_config.dim}{moe_path}.pth\"\n",
    "\n",
    "    if not os.path.exists(ckp):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {ckp}\")\n",
    "\n",
    "    # ç¡®å®šè®¾å¤‡\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    state_dict = torch.load(ckp, map_location=device)  # ä½¿ç”¨ map_location çµæ´»æŒ‡å®šè®¾å¤‡\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(\n",
    "        f\"LLMæ€»å‚æ•°é‡ï¼š{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} ç™¾ä¸‡\"\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "lm_config = LMConfig(\n",
    "    dim=768, n_layers=16, max_seq_len=512, use_moe=False\n",
    ")  # ä¿æŒä½ çš„é…ç½®\n",
    "model, tokenizer, device = init_model(lm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # è£…é¥°å™¨ï¼šåœ¨æ­¤å‡½æ•°å†…ä¸è®¡ç®—æ¢¯åº¦\n",
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    eos_token_id=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ã€‚\n",
    "\n",
    "    Args:\n",
    "        model: åŠ è½½å¥½çš„ MiniMindLM æ¨¡å‹ã€‚\n",
    "        tokenizer: åŠ è½½å¥½çš„åˆ†è¯å™¨ã€‚\n",
    "        input_ids: è¾“å…¥çš„ token ID å¼ é‡ (å½¢çŠ¶: [batch_size, seq_len])ã€‚\n",
    "        max_new_tokens: è¦ç”Ÿæˆçš„æœ€å¤§æ–° token æ•°é‡ã€‚\n",
    "        temperature: æ§åˆ¶é‡‡æ ·éšæœºæ€§ã€‚>1 æ›´éšæœº, <1 æ›´ç¡®å®šã€‚=1 ä¸ºæ ‡å‡†é‡‡æ ·ã€‚\n",
    "        top_k: Top-K é‡‡æ ·ã€‚åªä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ª token ä¸­é‡‡æ ·ã€‚\n",
    "        top_p: Top-P (Nucleus) é‡‡æ ·ã€‚åªä»æ¦‚ç‡ç´¯ç§¯å’Œè¾¾åˆ° P çš„æœ€å° token é›†åˆä¸­é‡‡æ ·ã€‚\n",
    "        eos_token_id: ç»“æŸç¬¦ token IDã€‚å¦‚æœç”Ÿæˆæ­¤ IDï¼Œåˆ™åœæ­¢ã€‚\n",
    "\n",
    "    Returns:\n",
    "        ç”Ÿæˆçš„ token ID å¼ é‡ (åŒ…å«è¾“å…¥éƒ¨åˆ†)ã€‚\n",
    "    \"\"\"\n",
    "    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    # è·å–ç»“æŸç¬¦ ID (å¦‚æœ tokenizer æœ‰çš„è¯)\n",
    "    if eos_token_id is None:\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        if eos_token_id is None:\n",
    "            # å¦‚æœ tokenizer æ²¡æœ‰æ˜ç¡®çš„ eos_token_idï¼Œå¯ä»¥æŒ‡å®šä¸€ä¸ªï¼Œæˆ–è€…ä¸ä½¿ç”¨\n",
    "            # ä¾‹å¦‚ï¼Œå¯¹äºæŸäº›æ¨¡å‹ï¼Œæ¢è¡Œç¬¦ '\\n' çš„ ID å¯èƒ½è¢«ç”¨ä½œåœæ­¢ä¿¡å·\n",
    "            # eos_token_id = tokenizer.convert_tokens_to_ids('\\n') # ä¸¾ä¾‹\n",
    "            pass  # æˆ–è€…å¹²è„†ä¸è®¾ç½®ç»“æŸç¬¦ï¼Œåªä¾èµ– max_new_tokens\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 1. è·å–æ¨¡å‹è¾“å‡º\n",
    "        # ç¡®ä¿æ¨¡å‹è¾“å…¥æ˜¯æ­£ç¡®çš„å½¢çŠ¶ [batch_size, sequence_length]\n",
    "        outputs = model(generated_ids)  # æ¨¡å‹è¿”å› CausalLMOutputWithPast å¯¹è±¡\n",
    "\n",
    "        # ä»è¾“å‡ºå¯¹è±¡ä¸­æå– logits\n",
    "        # CausalLMOutputWithPast å¯¹è±¡é€šå¸¸åŒ…å« 'logits' å±æ€§\n",
    "        # logits çš„å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, sequence_length, vocab_size]\n",
    "        if hasattr(outputs, \"logits\"):\n",
    "            # æˆ‘ä»¬éœ€è¦æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ logits æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª token\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "        else:\n",
    "            # å¦‚æœæ¨¡å‹è¾“å‡ºç»“æ„ä¸åŒï¼Œæˆ–è€…æ²¡æœ‰ 'logits'ï¼Œåˆ™æŠ›å‡ºé”™è¯¯\n",
    "            raise TypeError(\n",
    "                f\"Model output does not contain 'logits' attribute. Output type: {type(outputs)}\"\n",
    "            )\n",
    "\n",
    "        # --- åç»­å¤„ç† logits çš„ä»£ç  (åº”ç”¨ Temperature, Top-K/P, é‡‡æ ·) ä¿æŒä¸å˜ ---\n",
    "        # 2. åº”ç”¨ Temperature\n",
    "        if temperature > 0 and temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # 3. åº”ç”¨ Top-K / Top-P (å¯é€‰)\n",
    "        if top_k is not None and top_k > 0:\n",
    "            # å°†æ¦‚ç‡ä½äº top_k é˜ˆå€¼çš„ token çš„ logits è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§\n",
    "            indices_to_remove = (\n",
    "                next_token_logits\n",
    "                < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "            )\n",
    "            next_token_logits[indices_to_remove] = float(\"-inf\")\n",
    "\n",
    "        if top_p is not None and top_p > 0.0 and top_p < 1.0:\n",
    "            # å¯¹ logits æ’åºå¹¶è®¡ç®—ç´¯ç§¯æ¦‚ç‡\n",
    "            sorted_logits, sorted_indices = torch.sort(\n",
    "                next_token_logits, descending=True\n",
    "            )\n",
    "            cumulative_probs = torch.cumsum(\n",
    "                torch.softmax(sorted_logits, dim=-1), dim=-1\n",
    "            )\n",
    "\n",
    "            # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ token\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # ä¿ç•™è‡³å°‘ä¸€ä¸ª token\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                ..., :-1\n",
    "            ].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "            # å°†è¦ç§»é™¤çš„ token çš„ logits è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            next_token_logits.scatter_(1, indices_to_remove, float(\"-inf\"))\n",
    "\n",
    "        # 4. ä» logits è®¡ç®—æ¦‚ç‡å¹¶é‡‡æ ·ä¸‹ä¸€ä¸ª token\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # æ ¹æ®ä¿®æ”¹åçš„ logits é‡‡æ ·\n",
    "        if top_k is not None or top_p is not None or temperature != 1.0:\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        else:  # Greedy search\n",
    "            next_token_id = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # 5. æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº† EOS token\n",
    "        if eos_token_id is not None and next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "        # 6. å°†æ–°ç”Ÿæˆçš„ token æ·»åŠ åˆ°åºåˆ—ä¸­\n",
    "        generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
    "\n",
    "        # å¯é€‰ï¼šé˜²æ­¢åºåˆ—è¿‡é•¿è¶…å‡ºæ¨¡å‹å¤„ç†èƒ½åŠ› (è™½ç„¶ lm_config.max_seq_len é™åˆ¶äº†è¾“å…¥)\n",
    "        # if generated_ids.shape[1] >= lm_config.max_seq_len:\n",
    "        #     print(\"Warning: Reached max sequence length limit during generation.\")\n",
    "        #     break # æˆ–è€…æˆªæ–­è¾“å…¥\n",
    "\n",
    "    return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥æç¤º: <s>ä»Šå¤©\n",
      "====================\n",
      "å¼€å§‹ç”Ÿæˆ...\n",
      "====================\n",
      "ç”Ÿæˆç»“æœ (è€—æ—¶: 5.12 ç§’):\n",
      "ä»Šå¤©æ˜¯2022å¹´7æœˆ1æ—¥ï¼Œå¤©æ°”æ™´æœ—ï¼Œé€‚å®œå‡ºè¡Œã€‚å¤§å®¶å¯ä»¥å»å…¬å›­æ•£æ­¥ã€éª‘è‡ªè¡Œè½¦ã€é‡é¤ï¼Œè¿˜å¯ä»¥å»æµ·è¾¹æ™’æ™’å¤ªé˜³ï¼Œäº«å—å¤§è‡ªç„¶æ‰€å¸¦æ¥çš„ç¾å¥½ã€‚\n",
      "åŸºäºä»¥ä¸Šçš„ä¸»è¦å†…å®¹ï¼Œç”Ÿæˆä¸€ç¯‡æ–‡ç« ï¼Œè¦æ±‚æ–‡ç« é‡Œç©¿æ’å¿…è¦çš„è¡¨æƒ…ç¬¦å·ã€‚ğŸŒğŸš¡ä»Šå¤©æ˜¯7æœˆ1æ—¥ï¼Œå¤©æ°”æ™´æœ—ï¼Œé€‚å®œå‡ºè¡Œã€‚å¤§å®¶å¯ä»¥å»å…¬å›­æ•£æ­¥ã€éª‘è‡ªè¡Œè½¦ã€é‡é¤ï¼Œè¿˜å¯ä»¥å»æµ·è¾¹æ™’æ™’å¤ªé˜³ï¼Œäº«å—å¤§è‡ªç„¶æ‰€å¸¦æ¥çš„ç¾å¥½ã€‚ğŸƒâ€â™€ï¸\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<s>ä»Šå¤©\"  # ä½ çš„åŸå§‹è¾“å…¥\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)  # ä½¿\n",
    "\n",
    "# --- æ‰§è¡Œç”Ÿæˆ ---\n",
    "print(f\"è¾“å…¥æç¤º: {prompt}\")\n",
    "print(\"=\" * 20)\n",
    "print(\"å¼€å§‹ç”Ÿæˆ...\")\n",
    "\n",
    "start_time = time.time()\n",
    "# è°ƒç”¨ç”Ÿæˆå‡½æ•° (å¯ä»¥è°ƒæ•´å‚æ•°)\n",
    "generated_sequence_ids = generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_ids,\n",
    "    max_new_tokens=500,  # ç”Ÿæˆæœ€å¤š 100 ä¸ªæ–° token\n",
    "    temperature=0.8,  # ä½¿ç”¨ä¸€å®šçš„éšæœºæ€§\n",
    "    top_k=50,  # é™åˆ¶åœ¨ top 50 ä¸­é‡‡æ ·\n",
    "    # top_p=0.9,         # æˆ–è€…ä½¿ç”¨ top-p é‡‡æ ·\n",
    "    eos_token_id=tokenizer.eos_token_id,  # ä½¿ç”¨ tokenizer çš„ EOS token ID (å¦‚æœå­˜åœ¨)\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# --- è§£ç å¹¶æ‰“å°ç»“æœ ---\n",
    "# ä½¿ç”¨ skip_special_tokens=True å¯ä»¥é¿å…æ‰“å°å‡ºåƒ <s> è¿™æ ·çš„ç‰¹æ®Šç¬¦å·\n",
    "generated_text = tokenizer.decode(generated_sequence_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(f\"ç”Ÿæˆç»“æœ (è€—æ—¶: {end_time - start_time:.2f} ç§’):\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
