{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "LLM总参数量：104.031 百万\n"
     ]
    }
   ],
   "source": [
    "# 导入标准库\n",
    "import os  # 用于与操作系统交互，例如读取环境变量、创建目录\n",
    "import platform  # 用于获取操作系统平台信息 (虽然在此脚本中未直接使用其返回值)\n",
    "import argparse  # 用于解析命令行参数\n",
    "import time  # 用于计时，例如计算训练时长\n",
    "import math  # 用于数学运算，例如计算余弦学习率\n",
    "import warnings  # 用于控制警告信息的显示\n",
    "\n",
    "# 导入第三方库\n",
    "import pandas as pd  # 数据处理库 (虽然在此脚本中未直接使用，可能在导入的模块中使用或为未来扩展保留)\n",
    "import torch  # PyTorch核心库，用于张量计算和神经网络\n",
    "import torch.distributed as dist  # PyTorch分布式训练库 (在此测试脚本中未使用)\n",
    "from torch import optim, nn  # PyTorch优化器 (optim) 和神经网络模块 (nn)\n",
    "\n",
    "# from torch.nn.parallel import DistributedDataParallel # 不需要 DDP 进行单 GPU 推理\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    ")  # 余弦退火学习率调度器 (虽然未使用，但导入了)\n",
    "\n",
    "# from torch.utils.data import DataLoader, DistributedSampler # 不需要数据加载器进行简单生成\n",
    "from contextlib import nullcontext  # 用于创建空上下文管理器，方便在不同设备类型间切换\n",
    "\n",
    "# 导入Hugging Face Transformers库\n",
    "from transformers import AutoTokenizer  # 用于自动加载预训练模型的分词器\n",
    "\n",
    "import sys\n",
    "\n",
    "current_working_directory = os.getcwd()\n",
    "parent_directory = os.path.abspath(os.path.join(current_working_directory, \"..\"))\n",
    "sys.path.append(parent_directory)\n",
    "# 导入自定义模块\n",
    "from model.model import MiniMindLM  # 导入自定义的模型类 MiniMindLM\n",
    "from model.LMConfig import LMConfig  # 导入自定义的模型配置类 LMConfig\n",
    "\n",
    "# from model.dataset import PretrainDataset # 不需要数据集类进行简单生成\n",
    "\n",
    "\n",
    "# --- 你提供的初始化代码 ---\n",
    "def init_model(lm_config):\n",
    "    # 假设你的tokenizer文件确实在 ../model/minimind_tokenizer 目录下\n",
    "    tokenizer_path = \"../model/minimind_tokenizer\"\n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        # 如果路径不存在，尝试使用一个通用的预训练模型路径作为备选\n",
    "        # 或者直接报错提示用户检查路径\n",
    "        warnings.warn(\n",
    "            f\"Tokenizer path {tokenizer_path} not found. Trying a placeholder 'bert-base-uncased'. Adjust if needed.\"\n",
    "        )\n",
    "        tokenizer_path = \"bert-base-uncased\"  # 或者其他你知道存在的路径, 或者抛出错误\n",
    "        # raise FileNotFoundError(f\"Tokenizer path not found: {tokenizer_path}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    model = MiniMindLM(lm_config)\n",
    "    moe_path = \"_moe\" if lm_config.use_moe else \"\"\n",
    "    ckp = f\"../out/pretrain_{lm_config.dim}{moe_path}.pth\"\n",
    "\n",
    "    if not os.path.exists(ckp):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {ckp}\")\n",
    "\n",
    "    # 确定设备\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    state_dict = torch.load(ckp, map_location=device)  # 使用 map_location 灵活指定设备\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print(\n",
    "        f\"LLM总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万\"\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "lm_config = LMConfig(\n",
    "    dim=768, n_layers=16, max_seq_len=512, use_moe=False\n",
    ")  # 保持你的配置\n",
    "model, tokenizer, device = init_model(lm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # 装饰器：在此函数内不计算梯度\n",
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    eos_token_id=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    使用模型生成文本。\n",
    "\n",
    "    Args:\n",
    "        model: 加载好的 MiniMindLM 模型。\n",
    "        tokenizer: 加载好的分词器。\n",
    "        input_ids: 输入的 token ID 张量 (形状: [batch_size, seq_len])。\n",
    "        max_new_tokens: 要生成的最大新 token 数量。\n",
    "        temperature: 控制采样随机性。>1 更随机, <1 更确定。=1 为标准采样。\n",
    "        top_k: Top-K 采样。只从概率最高的 K 个 token 中采样。\n",
    "        top_p: Top-P (Nucleus) 采样。只从概率累积和达到 P 的最小 token 集合中采样。\n",
    "        eos_token_id: 结束符 token ID。如果生成此 ID，则停止。\n",
    "\n",
    "    Returns:\n",
    "        生成的 token ID 张量 (包含输入部分)。\n",
    "    \"\"\"\n",
    "    model.eval()  # 设置为评估模式\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    # 获取结束符 ID (如果 tokenizer 有的话)\n",
    "    if eos_token_id is None:\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        if eos_token_id is None:\n",
    "            # 如果 tokenizer 没有明确的 eos_token_id，可以指定一个，或者不使用\n",
    "            # 例如，对于某些模型，换行符 '\\n' 的 ID 可能被用作停止信号\n",
    "            # eos_token_id = tokenizer.convert_tokens_to_ids('\\n') # 举例\n",
    "            pass  # 或者干脆不设置结束符，只依赖 max_new_tokens\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 1. 获取模型输出\n",
    "        # 确保模型输入是正确的形状 [batch_size, sequence_length]\n",
    "        outputs = model(generated_ids)  # 模型返回 CausalLMOutputWithPast 对象\n",
    "\n",
    "        # 从输出对象中提取 logits\n",
    "        # CausalLMOutputWithPast 对象通常包含 'logits' 属性\n",
    "        # logits 的形状通常是 [batch_size, sequence_length, vocab_size]\n",
    "        if hasattr(outputs, \"logits\"):\n",
    "            # 我们需要最后一个时间步的 logits 来预测下一个 token\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "        else:\n",
    "            # 如果模型输出结构不同，或者没有 'logits'，则抛出错误\n",
    "            raise TypeError(\n",
    "                f\"Model output does not contain 'logits' attribute. Output type: {type(outputs)}\"\n",
    "            )\n",
    "\n",
    "        # 2. 应用 Temperature\n",
    "        if temperature > 0 and temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # 3. 应用 Top-K / Top-P (可选)\n",
    "        if top_k is not None and top_k > 0:\n",
    "            # 将概率低于 top_k 阈值的 token 的 logits 设置为负无穷大\n",
    "            indices_to_remove = (\n",
    "                next_token_logits\n",
    "                < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "            )\n",
    "            next_token_logits[indices_to_remove] = float(\"-inf\")\n",
    "\n",
    "        if top_p is not None and top_p > 0.0 and top_p < 1.0:\n",
    "            # 对 logits 排序并计算累积概率\n",
    "            sorted_logits, sorted_indices = torch.sort(\n",
    "                next_token_logits, descending=True\n",
    "            )\n",
    "            cumulative_probs = torch.cumsum(\n",
    "                torch.softmax(sorted_logits, dim=-1), dim=-1\n",
    "            )\n",
    "\n",
    "            # 移除累积概率超过 top_p 的 token\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # 保留至少一个 token\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                ..., :-1\n",
    "            ].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "            # 将要移除的 token 的 logits 设置为负无穷大\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            next_token_logits.scatter_(1, indices_to_remove, float(\"-inf\"))\n",
    "\n",
    "        # 4. 从 logits 计算概率并采样下一个 token\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # 根据修改后的 logits 采样\n",
    "        if top_k is not None or top_p is not None or temperature != 1.0:\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        else:  # Greedy search\n",
    "            next_token_id = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # 5. 检查是否生成了 EOS token\n",
    "        if eos_token_id is not None and next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "        # 6. 将新生成的 token 添加到序列中\n",
    "        generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
    "\n",
    "        # 可选：防止序列过长超出模型处理能力 (虽然 lm_config.max_seq_len 限制了输入)\n",
    "        # if generated_ids.shape[1] >= lm_config.max_seq_len:\n",
    "        #     print(\"Warning: Reached max sequence length limit during generation.\")\n",
    "        #     break # 或者截断输入\n",
    "\n",
    "    return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入提示: <s>今天\n",
      "====================\n",
      "开始生成...\n",
      "====================\n",
      "生成结果 (耗时: 1.87 秒):\n",
      "今天天气不错，出去散步。今天天气不错，出去散步。\n",
      "回答上面的问题。这道题目要求学生在阅读题目时理解题意，并根据题意得出正确的答案。题目中并没有明确给出今天的天气情况，因此我们无法得知今天是否下雨或有其他的降雨可能性。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<s>今天\"  # 你的原始输入\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)  # 使\n",
    "\n",
    "# --- 执行生成 ---\n",
    "print(f\"输入提示: {prompt}\")\n",
    "print(\"=\" * 20)\n",
    "print(\"开始生成...\")\n",
    "\n",
    "start_time = time.time()\n",
    "# 调用生成函数 (可以调整参数)\n",
    "generated_sequence_ids = generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_ids,\n",
    "    max_new_tokens=500,  # 生成最多 100 个新 token\n",
    "    temperature=0.8,  # 使用一定的随机性\n",
    "    top_k=50,  # 限制在 top 50 中采样\n",
    "    # top_p=0.9,         # 或者使用 top-p 采样\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 使用 tokenizer 的 EOS token ID (如果存在)\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# --- 解码并打印结果 ---\n",
    "# 使用 skip_special_tokens=True 可以避免打印出像 <s> 这样的特殊符号\n",
    "generated_text = tokenizer.decode(generated_sequence_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(f\"生成结果 (耗时: {end_time - start_time:.2f} 秒):\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
